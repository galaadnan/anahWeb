from optimum.onnxruntime import ORTModelForSequenceClassification
from transformers import AutoTokenizer
import os
import torch

# Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
model_path = "./ai model/UBC-NLP_MARBERTv2"
output_path = "./ai model/UBC-NLP_MARBERTv2/onnx"

print("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¨Ù†Ù…Ø· Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø°Ø§ÙƒØ±Ø© Ù…Ù†Ø®ÙØ¶...")

try:
    # Ø£Ø¶ÙÙ†Ø§ low_cpu_mem_usage=True Ù„ØªØ¬Ù†Ø¨ Ø®Ø·Ø£ bad allocation
    model = ORTModelForSequenceClassification.from_pretrained(
        model_path, 
        export=True,
        low_cpu_mem_usage=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    model.save_pretrained(output_path)
    tokenizer.save_pretrained(output_path)

    old_file = os.path.join(output_path, "model.onnx")
    new_file = os.path.join(output_path, "model_quantized.onnx")
    
    if os.path.exists(old_file):
        if os.path.exists(new_file): os.remove(new_file)
        os.rename(old_file, new_file)
        print("ğŸ”„ ØªÙ… ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­.")

    print(f"âœ… Ù…Ø¨Ø±ÙˆÙƒ! ØªÙ… Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ: {output_path}")

except Exception as e:
    print(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {e}")